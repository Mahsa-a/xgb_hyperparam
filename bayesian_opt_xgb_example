from utils import optimize_xgb
from utils import save_obj
import xgboost as xgb

# loading training data
dtrain = xgb.DMatrix('/train.buffer')
dtrain.get_base_margin()

# create the hyper parameter space
pbounds = {'max_depth': (2, 6),
           'gamma': (0.001, 10.0),
           'min_child_weight': (5, 50),
           'subsample': (0.6, 1.0),
           'colsample_bytree': (0.4, 1.0),
           'learning_rate': (0.01, 0.1),
           'tweedie_variance_power': (1.6, 1.9)}

# run Bayesian Optimization with a few initial points
optimize_xgb(data=dtrain, num_boost_round=500, pbounds=pbounds, n_iter=50, acq='ei', kappa=3, init_points=10)

# Try more kappa options for better exploration 
optimize_xgb(data=dtrain, pbounds=pbounds, n_iter=50, acq='ei', kappa=10, init_points=10)
optimize_xgb(data=dtrain, pbounds=pbounds, n_iter=50, acq='ucb', kappa=1, init_points=10)
